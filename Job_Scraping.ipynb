{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jobspy in c:\\users\\hrith\\projects\\jd generator agent\\.jd_suggestor\\lib\\site-packages (0.29.0)\n",
      "Requirement already satisfied: redis>=3.0.0 in c:\\users\\hrith\\projects\\jd generator agent\\.jd_suggestor\\lib\\site-packages (from jobspy) (6.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install jobspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jobspy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseModel, ValidationError\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjobspy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m scrape_jobs\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'jobspy'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "robust_jd_pipeline.py\n",
    "-----------------------------------------\n",
    "Scrape, clean, LLM-parse, vector-embed & store\n",
    "job descriptions that match a hiring\n",
    "managerâ€™s brief.\n",
    "\n",
    "Dependencies  (pip install â€¦)\n",
    "--------------------------------\n",
    "jobspy==0.4.*\n",
    "beautifulsoup4\n",
    "requests\n",
    "sentence-transformers\n",
    "chromadb\n",
    "ollama-python       # or llama-cpp-python\n",
    "tqdm\n",
    "pydantic\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import re, os, json, random, logging, pathlib, datetime as dt\n",
    "from typing import Sequence, Tuple\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pydantic import BaseModel, ValidationError\n",
    "from jobspy import scrape_jobs\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "OLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\n",
    "OLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", \"phi2\")            # pre-pulled via `ollama pull phi2`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_extract(text: str) -> dict[str, str | int | list[str]]:\n",
    "    \"\"\"\n",
    "    Feed a JD and get back a JSON dict with {title, company, exp_years, skills, â€¦}.\n",
    "    The pydantic schema (below) guarantees type safety.\n",
    "    \"\"\"\n",
    "    system = (\n",
    "        \"You extract structured info from job ads. \"\n",
    "        \"Return *ONLY* valid JSON matching this schema:\\n\"\n",
    "        \"{\"\n",
    "        '\"title\": str, \"company\": str, \"experience_years\": int, '\n",
    "        '\"location\": str, \"skills\": [str], \"education\": str'\n",
    "        \"}\"\n",
    "    )\n",
    "    prompt = f\"{system}\\n\\n### JOB AD\\n{text}\\n\\n### JSON:\"\n",
    "    resp = requests.post(\n",
    "        f\"{OLLAMA_HOST}/api/generate\",\n",
    "        json={\"model\": OLLAMA_MODEL, \"prompt\": prompt, \"stream\": False, \"options\": {\"temperature\": 0.0}},\n",
    "        timeout=90,\n",
    "    )\n",
    "    resp.raise_for_status()\n",
    "    raw = resp.json()[\"response\"].strip()\n",
    "    # guard: sometimes model wraps in markdown code-fence\n",
    "    raw = raw.lstrip(\"`\").rstrip(\"`\")\n",
    "    try:\n",
    "        return json.loads(raw)\n",
    "    except json.JSONDecodeError:\n",
    "        logging.warning(\"LLM returned invalid JSON, skipping.\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pydantic schema to validate / coerce\n",
    "class JD(BaseModel):\n",
    "    title: str\n",
    "    company: str\n",
    "    experience_years: int | None = None\n",
    "    location: str | None = None\n",
    "    skills: list[str] = []\n",
    "    education: str | None = None\n",
    "    description: str                   # keep full text\n",
    "    url: str\n",
    "    posted_dt: dt.datetime\n",
    "    search_term: str\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2. Embedding & Vector DB (Chroma, local)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "EMB_MODEL = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "CHROMA_DIR = pathlib.Path(\"data/jd_vectors\").as_posix()\n",
    "client = chromadb.PersistentClient(path=CHROMA_DIR)\n",
    "collection = client.get_or_create_collection(name=\"job_descriptions\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3. Main pipeline\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def build_and_store_jds(\n",
    "    job_title: str,\n",
    "    exp_range: Tuple[int, int],\n",
    "    location: str = \"United States\",\n",
    "    sector_keywords: Sequence[str] | None = None,\n",
    "    hours_old: int = 72,\n",
    "    results_each: int = 50,\n",
    "    extra_google: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    â€¢ Scrape LinkedIn (JobSpy)  [+ optional Google Jobs]\n",
    "    â€¢ Clean & de-dup\n",
    "    â€¢ LLM-parse â†’ JD schema\n",
    "    â€¢ Filter by experience range & sector\n",
    "    â€¢ Embed + upsert into Chroma\n",
    "    Returns final DataFrame of stored docs\n",
    "    \"\"\"\n",
    "    # 1  SCRAPE  â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“\n",
    "    frames: list[pd.DataFrame] = []\n",
    "    print(\"Scraping LinkedIn via JobSpyâ€¦\")\n",
    "    df_ln = scrape_jobs(\n",
    "        site_name=\"linkedin\",\n",
    "        search_term=job_title,\n",
    "        location=location,\n",
    "        job_type=\"fulltime\",\n",
    "        hours_old=hours_old,\n",
    "        results_wanted=results_each,\n",
    "        linkedin_fetch_description=True,\n",
    "        description_format=\"markdown\",\n",
    "    )\n",
    "    frames.append(pd.DataFrame(df_ln))\n",
    "\n",
    "    if extra_google:\n",
    "        print(\"Scraping Google Jobs via JobSpyâ€¦\")\n",
    "        df_gg = scrape_jobs(\n",
    "            site_name=\"google\",\n",
    "            search_term=job_title,\n",
    "            location=location,\n",
    "            hours_old=hours_old,\n",
    "            results_wanted=results_each,\n",
    "            description_format=\"markdown\",\n",
    "        )\n",
    "        frames.append(pd.DataFrame(df_gg))\n",
    "\n",
    "    raw = pd.concat(frames, ignore_index=True)\n",
    "    raw[\"posted_dt\"] = pd.to_datetime(raw[\"posted_time\"])\n",
    "    raw[\"description\"] = raw[\"description\"].fillna(\"\")\n",
    "\n",
    "    # 2  DE-DUP  â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“\n",
    "    raw = (\n",
    "        raw.sort_values(\"posted_dt\", ascending=False)\n",
    "        .drop_duplicates(subset=\"job_url\", keep=\"first\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    print(f\"{len(raw):,} unique ads after URL dedup.\")\n",
    "\n",
    "    # 3  LLM PARSE  â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“\n",
    "    records: list[dict] = []\n",
    "    for _, row in tqdm(raw.iterrows(), total=len(raw), desc=\"LLM parse\"):\n",
    "        info = llm_extract(row[\"description\"])\n",
    "        if not info:\n",
    "            continue\n",
    "        try:\n",
    "            jd = JD(\n",
    "                **info,\n",
    "                description=row[\"description\"],\n",
    "                url=row[\"job_url\"],\n",
    "                posted_dt=row[\"posted_dt\"],\n",
    "                search_term=row[\"search_term\"],\n",
    "            )\n",
    "            records.append(jd.model_dump())\n",
    "        except ValidationError:\n",
    "            continue\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    print(f\"{len(df):,} ads passed JSON validation.\")\n",
    "\n",
    "    # 4  FILTER  (by exp & sector keywords) â€“â€“â€“â€“â€“â€“â€“\n",
    "    low, high = exp_range\n",
    "    if df.empty:\n",
    "        print(\"No valid JDs to filter.\")\n",
    "        return df\n",
    "\n",
    "    df = df[df[\"experience_years\"].between(low, high, inclusive=\"both\")]\n",
    "    if sector_keywords:\n",
    "        pat = re.compile(\"|\".join(map(re.escape, sector_keywords)), re.I)\n",
    "        df = df[\n",
    "            df[\"description\"].str.contains(pat)\n",
    "            | df[\"company\"].str.contains(pat, case=False, na=False)\n",
    "        ]\n",
    "    print(f\"{len(df):,} ads after exp & sector filters.\")\n",
    "\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    # 5  EMBED & UPSERT  â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“\n",
    "    embeds = EMB_MODEL.encode(df[\"description\"].tolist(), show_progress_bar=True)\n",
    "    # ensure deterministic ids\n",
    "    ids = [f\"{hash(url)%2**63}\" for url in df[\"url\"]]\n",
    "    metadata = df.drop(columns=[\"description\"]).to_dict(orient=\"records\")\n",
    "\n",
    "    collection.upsert(\n",
    "        ids=ids,\n",
    "        embeddings=embeds,\n",
    "        metadatas=metadata,\n",
    "        documents=df[\"description\"].tolist(),\n",
    "    )\n",
    "    print(f\"Upserted {len(ids)} vectors into Chroma â†’ {CHROMA_DIR}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# â”€â”€ Example call â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if __name__ == \"__main__\":\n",
    "    df_final = build_and_store_jds(\n",
    "        job_title=\"Procurement Manager\",\n",
    "        exp_range=(12, 15),\n",
    "        location=\"United States\",\n",
    "        sector_keywords=[\"automotive\", \"OEM\", \"vehicle\", \"tier-1\"],\n",
    "        hours_old=168,                      # 7 days for niche roles\n",
    "        results_each=60,\n",
    "    )\n",
    "    print(df_final.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('scrpaed_jobs.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import openai\n",
    "import json\n",
    "from docx import Document\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "env_path = 'C:\\Outlook_Email_Generator\\config.env'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Set your OpenAI API key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "job_title_folder_map = {\n",
    "    # Data Science\n",
    "    \"data scientist\": \"Data_Scientist\",\n",
    "    \"data science\": \"Data_Scientist\",\n",
    "    \"research scientist\": \"Data_Scientist\",\n",
    "    \"applied scientist\": \"Data_Scientist\",\n",
    "    \"scientist\": \"Data_Scientist\",\n",
    "\n",
    "    # Data Engineering\n",
    "    \"data engineer\": \"Data_Engineer\",\n",
    "    \"data engineering\": \"Data_Engineer\",\n",
    "    \"etl engineer\": \"Data_Engineer\",\n",
    "    \"big data\": \"Data_Engineer\",\n",
    "    \"pipeline engineer\": \"Data_Engineer\",\n",
    "    \"sql engineer\": \"Data_Engineer\",\n",
    "    \"database engineer\": \"Data_Engineer\",\n",
    "\n",
    "    # Machine Learning\n",
    "    \"ml engineer\": \"ML_Engineer\",\n",
    "    \"machine learning\": \"ML_Engineer\",\n",
    "    \"ai engineer\": \"ML_Engineer\",\n",
    "    \"artificial intelligence\": \"ML_Engineer\",\n",
    "    \"generative ai\": \"ML_Engineer\",\n",
    "    \"gen ai\": \"ML_Engineer\",\n",
    "    \"deep learning\": \"ML_Engineer\",\n",
    "    \"computer vision\": \"ML_Engineer\",\n",
    "    \"llm\": \"ML_Engineer\",\n",
    "    \"nlp\": \"ML_Engineer\",\n",
    "    \"mlops\": \"ML_Engineer\",\n",
    "    \"ml researcher\": \"ML_Engineer\",\n",
    "\n",
    "    # Data Analytics / BI\n",
    "    \"data analyst\": \"Data_Analyst\",\n",
    "    \"business analyst\": \"Data_Analyst\",\n",
    "    \"bi analyst\": \"Data_Analyst\",\n",
    "    \"business intelligence\": \"Data_Analyst\",\n",
    "    \"data analytics\": \"Data_Analyst\",\n",
    "    \"analytics\": \"Data_Analyst\",\n",
    "    \"analyst\": \"Data_Analyst\",\n",
    "    \"reporting analyst\": \"Data_Analyst\",\n",
    "    \"sql analyst\": \"Data_Analyst\",\n",
    "    \"marketing analyst\": \"Data_Analyst\",\n",
    "    \"insights analyst\": \"Data_Analyst\"\n",
    "}\n",
    "\n",
    "# --- Step 1: Extract resume text ---\n",
    "def extract_text_from_docx(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    return \"\\n\".join([para.text.strip() for para in doc.paragraphs if para.text.strip()])\n",
    "\n",
    "def load_resumes(folder_path):\n",
    "    resumes = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".docx\"):\n",
    "            full_path = os.path.join(folder_path, file)\n",
    "            text = extract_text_from_docx(full_path)\n",
    "            resumes.append({\"resume_path\": full_path, \"resume_text\": text})\n",
    "    return resumes\n",
    "\n",
    "def resolve_resume_folder(base_folder, job_title):\n",
    "    job_title = job_title.lower()\n",
    "    for keyword, folder in job_title_folder_map.items():\n",
    "        if keyword in job_title:\n",
    "            return os.path.join(base_folder, folder), folder\n",
    "    return None, None\n",
    "\n",
    "# --- Step 2: GPT-4 Prompt ---\n",
    "def find_best_resume_and_experience(job_title, company, job_description, resumes):\n",
    "    resume_chunks = \"\\n\\n\".join([\n",
    "        f\"Resume {i+1}:\\nPath: {r['resume_path']}\\nText: {r['resume_text']}\" for i, r in enumerate(resumes)\n",
    "    ])\n",
    "    client = openai.OpenAI(api_key=api_key)\n",
    "    prompt = f\"\"\"\n",
    "        You are an experienced technical recruiter.\n",
    "\n",
    "        You have two tasks:\n",
    "\n",
    "        **Task 1: Resume Match**\n",
    "        Evaluate the following {len(resumes)} candidate resumes and select the ONE that is most suitable for the job described below.\n",
    "\n",
    "        **Task 2: Experience Extraction**\n",
    "        Carefully read the job description and extract the required years of experience **only if it is explicitly stated**.\n",
    "\n",
    "        The experience must be formatted in one of the following ways:\n",
    "        - A single integer, e.g., `\"3\"` â†’ means exactly 3 years\n",
    "        - A range in the format `\"0-3\"` â†’ means 0 to 3 years\n",
    "        - A minimum threshold in the format `\"3+\"` â†’ means 3 or more years\n",
    "\n",
    "        If no such requirement is mentioned clearly in the job description, return `null`.\n",
    "\n",
    "\n",
    "        ---\n",
    "\n",
    "        **Job Title:** {job_title}\n",
    "        **Company:** {company}\n",
    "        **Job Description:**\n",
    "        {job_description}\n",
    "\n",
    "        ---\n",
    "\n",
    "        **Candidate Resumes:**\n",
    "        {resume_chunks}\n",
    "\n",
    "        ---\n",
    "\n",
    "        **Return your answer in the following JSON format:**\n",
    "        {{ \"best_resume_path\": \"<resume_path>\", \"required_experience_years\": \"<3 | 0-3 | 3+ | null>\", \"experience_source\": \"extracted\" | \"none\" }}\n",
    "\n",
    "        - Take your time and think like a technical  recruiter before deciding the best resume path\n",
    "        - Only use `\"extracted\"` if the experience is clearly stated in the job description.\n",
    "        - Use `\"none\"` if no specific requirement is mentioned.\n",
    "        - Do not guess based on job title or seniority.\n",
    "\n",
    "        Return only the JSON response, nothing else.\n",
    "        \"\"\"   \n",
    "    print('Here')\n",
    "    try:\n",
    "        response = client.chat.completions.create( \n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3\n",
    "        )\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        if content.startswith(\"```json\"):\n",
    "            content = content.replace(\"```json\", \"\").strip()\n",
    "        if content.endswith(\"```\"):\n",
    "            content = content[:-3].strip()\n",
    "        \n",
    "        # Print the raw content for debugging (optional)\n",
    "        print(\"ğŸ” Cleaned GPT Output:\\n\", content)\n",
    "        \n",
    "        # Try parsing JSON\n",
    "        result = json.loads(content)\n",
    "        \n",
    "        return result[\"best_resume_path\"], result[\"required_experience_years\"], result[\"experience_source\"]\n",
    "\n",
    "    except json.JSONDecodeError as jde:\n",
    "        print(\"âŒ JSON parse error:\", jde)\n",
    "        print(\"âš ï¸ GPT response was:\", content)\n",
    "        return None, None, None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ GPT call failed: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# --- Step 3: Annotate jobs with best resume ---\n",
    "# Master function to attach resume and experience info to job DataFrame\n",
    "def match_resumes_to_jobs(job_df, base_resume_folder):\n",
    "    best_paths, exp_years_list, exp_sources = [], [], []\n",
    "\n",
    "    for index, row in tqdm(job_df.iterrows(), total=len(job_df), desc=\"Matching resumes\"):\n",
    "        job_title = row.get(\"title\", \"\")\n",
    "        company = row.get(\"company\", \"\")\n",
    "        description = row.get(\"description\", \"\")\n",
    "\n",
    "        if not all([job_title, company, description]) or pd.isna(job_title) or pd.isna(company) or pd.isna(description):\n",
    "            best_paths.append(None)\n",
    "            exp_years_list.append(None)\n",
    "            exp_sources.append(None)\n",
    "            #folders_used.append(None)\n",
    "            continue\n",
    "\n",
    "        resume_folder, folder_name = resolve_resume_folder(base_resume_folder, job_title)\n",
    "\n",
    "        if not resume_folder or not os.path.exists(resume_folder):\n",
    "            print(f\"âš ï¸ Skipping row {index} â€” no folder match for: {job_title}\")\n",
    "            best_paths.append(None)\n",
    "            exp_years_list.append(None)\n",
    "            exp_sources.append(None)\n",
    "            #folders_used.append(None)\n",
    "            continue\n",
    "\n",
    "        resumes = load_resumes(resume_folder)\n",
    "\n",
    "        if not resumes:\n",
    "            best_paths.append(None)\n",
    "            exp_years_list.append(None)\n",
    "            exp_sources.append(None)\n",
    "            #folders_used.append(folder_name)\n",
    "            continue\n",
    "\n",
    "        best_path, exp_years, exp_source = find_best_resume_and_experience(job_title, company, description, resumes)\n",
    "\n",
    "        best_paths.append(best_path)\n",
    "        exp_years_list.append(exp_years)\n",
    "        exp_sources.append(exp_source)\n",
    "        #folders_used.append(folder_name)\n",
    "\n",
    "    job_df[\"best_resume_path\"] = best_paths\n",
    "    job_df[\"required_experience_years\"] = exp_years_list\n",
    "    job_df[\"experience_source\"] = exp_sources\n",
    "    #job_df[\"resume_folder_used\"] = folders_used\n",
    "\n",
    "    return job_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df = df.head(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # From your jobspy scraping code\n",
    "job_df=jobs_df.copy()\n",
    "resumes_folder = \"Resumes\"\n",
    "updated_df = match_resumes_to_jobs(jobs_df, resumes_folder)\n",
    "#updated_df.to_csv(\"jobs_with_best_resumes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df.loc[0,'description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".jd_suggestor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
